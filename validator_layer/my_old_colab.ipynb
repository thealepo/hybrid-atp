{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4df697",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'venv (Python 3.11.9)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Alex E. Sanchez/Desktop/Projects/MAIN/hybrid-ns-theorem-prover/hybrid-atp/venv/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "from datasets import Dataset\n",
    "from typing import List , Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c88fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/abs/2109.08203\n",
    "random.seed(3407)\n",
    "np.random.seed(3407)\n",
    "torch.manual_seed(3407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923c6d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lean\n",
    "%pip install lean_dojo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f6a36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lean_dojo import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c86f849",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = LeanGitRepo(\n",
    "    'https://github.com/leanprover-community/mathlib4',\n",
    "    ''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b8a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_repo = trace(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763db224",
   "metadata": {},
   "outputs": [],
   "source": [
    "theorems = traced_repo.get_traced_theorems()\n",
    "print(f'{len(theorems)} theorems traced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b765481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_tactic_pairs = []\n",
    "\n",
    "for theorem in tqdm(theorems):\n",
    "  for t in theorem.get_traced_tactics():\n",
    "    static_tactic_pairs.append(\n",
    "        {'state': t.state_before,\n",
    "         'tactic': t.tactic\n",
    "         })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29011e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = static_tactic_pairs[0]\n",
    "print(st['state'])\n",
    "print(st['tactic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2b27e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained('google/byt5-small')\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/byt5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a257e2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_list(state_tactic_pairs).shuffle().select(range(10000))\n",
    "\n",
    "def tokenize(examples):\n",
    "  model_inputs = tokenizer(examples['state'] , max_length=2048 , truncation=True)\n",
    "  labels = tokenizer(test_target=examples['tactic'] , max_length=2048 , truncation=True)\n",
    "  model_inputs['labels'] = labels['input_ids']\n",
    "  return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize , batched=True)\n",
    "print(tokenized_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee334f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example , dont run\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    max_steps=2,\n",
    "    use_cpu=True\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer , model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99661ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('kaiyuy/leandojo-lean4-tacgen-byt5-small')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('kaiyuy/leandojo-lean4-tacgen-byt5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dd31a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tactic(state: str) -> str:\n",
    "  tokenized_state = tokenizer(state , return_tensor='pt')\n",
    "  tactic_ids = model.generate(tokenized_state.input_ids , max_length=1024)\n",
    "  tactic = tokenizer.decode(tactic_ids[0] , skip_special_tokens=True)\n",
    "  print(tactic , end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a557217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tacticS(state: str , k: int = 16) -> List[str]:\n",
    "  tokenized_state = tokenizer(state , return_tensor='pt')\n",
    "  tactic_candidates_ids = model.generate(\n",
    "      tokenized_state.input_ids ,\n",
    "      max_length=256,\n",
    "      num_beams=k,\n",
    "      length_penalty=0.0,\n",
    "      do_sample=False,\n",
    "      num_return_sequences=k,\n",
    "      output_scores=True,\n",
    "      early_stopping=True\n",
    "  )\n",
    "  tactic_candidates = tokenizer.batch_decode(\n",
    "      tactic_candidates_ids,\n",
    "      skip_special_tokens=True\n",
    "  )\n",
    "  return tactic_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547c0a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = LeanGitRepo(\n",
    "    'https://github.com/yangky11/lean4-example',\n",
    "    ''\n",
    ")\n",
    "theorem = Theorem(repo , 'Lean4Example.lean' , 'add_abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc6347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tactic = str\n",
    "Proof = List[Tactic]\n",
    "\n",
    "num_candidates = 16\n",
    "depth_limits = 3\n",
    "\n",
    "def search(state: TacticState , depth: int) -> Optional(Proof):\n",
    "  if depth >= depth_limit:\n",
    "    return None\n",
    "\n",
    "  tactics = generate_tacticS(state.pp , num_candidates)\n",
    "\n",
    "  for tac in tactics:\n",
    "    next_state = dojo.run_tac(state , tac)\n",
    "    if isinstance(next_state , ProofFinished):\n",
    "      return [tac] # found proof\n",
    "    elif not isinstance(next_state , LeanError):\n",
    "      # recursive dfs'\n",
    "      subproof = search(next_state , depth + 1)\n",
    "      if subproof is not None:\n",
    "        return [tac] + subproof\n",
    "\n",
    "  return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab6306e",
   "metadata": {},
   "source": [
    "Basically works by:\n",
    "- LeanDojo extracts state-tactic pairs from mathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6861cb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
